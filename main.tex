\documentclass[11pt, a4paper]{article} %\documentclass[11pt, a4paper, twoside, openright]{article} %draft

\usepackage{graphicx,color}
\usepackage{amssymb, amsmath, array}
\usepackage{hyperref}
\usepackage{newpxtext, newpxmath}
\usepackage{minted}
\usepackage{xcolor}
\usemintedstyle{friendly}

\begin{document}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\onecolumn

\input{cover}


\thispagestyle{empty}

\newpage

\tableofcontents{\protect\thispagestyle{empty}

\clearpage

\section{Introduction}

With the Internet cemented as a principal keystone in communication, and ever
increasing activity taking place digitially, the importance of secure
communication and Cryptography has never been greater. Thankfully, after
50 years of Public Key Cryptography
\cite{hellman_overview_1978},
we have good theoretical systems to provide these guarantees.

Most of these systems rely on modular arithmetic with large numbers,
such as RSA or Elliptic Curve Cryptography
\cite{rivest_method_1978, miller_use_1986}
.
Working with such numbers is not natively supported by hardware,
requiring a "Big Number" software library to provide this functionality.

Unfortunately, even though Public Key Cryptosystems have been heavily
scrutinized \textit{in theory}, in practice many vulnerabilities arise
in software implementations of these systems.

One particularly pernicious class of vulnerability are
\textbf{timing attacks}
\cite{kocher_cryptanalysis_1995}, where an implementation leaks information
about secret values through its execution time or cache usage, among
many side-channels.

Libraries for Big Numbers that are not designed with Cryptography in mind
are pervasively vulnerable to this class of attack.

In particular, Go \cite{the_go_authors_go_nodate} provides a general purpose
Big Number type, \texttt{big.Int}, which suffers from these vulnerabilities,
as we detail later in this report. Unfortunately, this library
gets used for Cryptography
\cite{ford_proposal_2017}, including inside of Go's own standard library,
in \texttt{go/crypto}.

We've addressed this issue by creating a library
\cite{meier_cronokirbysafenum_2021}
designed to work with Big Numbers in the context of Public Key Cryptography.
Our library provides the necessary operations for implementing these systems,
all while avoiding the leakage of secret information.
To demonstrate its utility, we've modified Go's \texttt{go/crypto}
package, replacing the use of \texttt{big.Int} in the DSA and RSA
systems.

\section{Background}

In this section, we explain how Big Numbers are used in Public Key
Cryptography, what timing attacks are, and how they affect our threat
model, as well as what kind of side-channels are present in Go's
\texttt{big.Int} type.

\subsection{Big Numbers in Cryptography}

As mentioned previously, most Public Key Cryptosystems rely on
modular arithmetic.

In RSA \cite{rivest_method_1978}, for example, a public key $(e, N)$ consists of modulus
$N \in \mathbb{N}$, usually 2048 bits long, and an exponent taken
modulo $\varphi(N)$.
To encrypt a message $m \in \mathbb{Z}/N\mathbb{Z}$, we calculate

$$
c := m^e \mod N
$$

The typical word size on computers is now 64 bits. Because of this,
to do arithmetic modulo $N$, we need a Big Number library to work
with these large numbers, as well as to provide optimized implementations
of opereaitons like modular exponentiation, which aren't natively supported.

DSA
\cite{technology_digital_1994} also relies on modular arithmetic,
this time using a large prime $p$ of around 2048 bits, and working
in the multiplicative group $(\mathbb{Z}/p\mathbb{Z})^*$.

Elliptic Curve Cryptography
\cite{miller_use_1986} relies on complex formulas for adding points
on an elliptic curve, built over a finite field $K$. This field
is usually either a prime field $\mathbb{Z}/p\mathbb{Z}$, in which case arithmetic
modulo $p$ is used, or a
binary extension field $\text{GF}(2^n)$, in which case
binary arithmetic in combination with polynomial addition and multiplication
are used.

For prime fields, a Big Number library is once again necessary, because
the size of the prime is greater than 200 bits.

In summary, large modular arithmetic is a cornerstone of Public Key
cryptosystems, requiring Big Numbers in some form.

\subsubsection{Implementing Big Numbers}

When you know the modulus in advance, like for Elliptic Curve Cryptography,
where prime modulus is part of the system itself, then you
can implement a library for doing arithmetic with that specific modulus.
By using a fixed size type, it's easier to provide constant-time operation
as well.

The downside is that since different systems require different moduli,
a lot of effort has to be expended to implement and support
these different systems.

One way to address this is to automatically generate implementations
of modular arithmetic, as done by FiatCrypto
\cite{hvass_high-assurance_nodate}.

In some systems, you need support for dynamic moduli. For example,
in RSA. In this case, you do need a Big Number libary of some kind,
providing dynamically sized numbers, and a myriad of operations.


\subsubsection{Big Numbers in \texttt{go/crypto}}

Go \cite{the_go_authors_go_nodate} provides implementations of numerous
cryptographic algorithms as part of its standard library,
including the aforementioned Public Key systems, in the
\texttt{go/crypto} package.

Unfortunately \cite{ford_proposal_2017}, the general purpose
\texttt{big.Int} type gets used in this package for Cryptography,
despite its potential vulnerability to timing attacks.

For DSA \cite{technology_digital_1994}, Go uses \texttt{big.Int}
for all operations, including key generation, signing, and verification.

For RSA \cite{rivest_method_1978}, Go uses \texttt{big.Int}
for all operations, and fixes this type as part of the API for
this package. Key generation, encryption, decryption, signing,
and verification all use \texttt{big.Int}.

For ECC \cite{miller_use_1986}, Go defines a general interface
for Elliptic Curves, requiring operations like point addition,
scalar multiplication, etc. All of these are defined in terms
of \texttt{big.Int}. Some of the curves have specialized
implementations of their prime field, only converting from
\texttt{big.Int} to satisfy the interface curves. The remaining curves
directly make use \texttt{big.Int} for their field arithmetic.

\subsection{Timing Attacks}

A side-channel
\cite{kelsey_side_1998}
leaks secret information about a program not directly,
but indirectly, through the observable properties of its execution.
For example, timing side-channels use the execution time of a program
to infer properties of the secret data it processes. 
A timing attack is the use of a timing side-channel to break the security
of some program or cryptographic protocol.

If a program takes a different number of steps based on the value of some
secret, then this constitutes an obvious timing side-channel.
For example, if a naive program for comparing inputs with a secret password
stops as soon as a mismatch is found, then the algorithm itself has
a timing side-channel. This side-channel can be exploited, to allow
the secret password to be guessed byte by byte.

Not all timing side-channels are this simple, however. Algorithms
that take the same number of steps regardless of the value of secret
data can still having timing side-channels because of how the underlying
hardware executing the program works. For example, a processor may
execute an operation faster for some inputs, or the presence of a cache
could be used to infer what addresses are being accessed. These
microarchitectural timing side-channels are also of concern.
See \cite{ge_survey_2018} for a survey of these vulnerabilities.

\subsubsection{Actual Attacks}

The presence of a side-channel does not directly lead to vulnerabilities.
As early as 1995, Paul Kocher
demonstrated the potential for timing attacks against cryptographic
algorithms \cite{kocher_cryptanalysis_1995, kocher_timing_1996}.
These specific attacks rely on algorithms that perform a varying number of
operations based on secret data. 

One common objection to timing attacks more generally is that they
while a timing side-channel is catastrophic in theory, in practice
this channel is too noisy to exploit. Unfortunately, through gathering
many samples, it's possible to exploit these attacks even across
a network \cite{brumley_remote_2005, brumley_remote_2011}.

The use of caches as a potential side-channel was
identified early on as well \cite{page_theoretical_2002}.
The idea is that accessing data that is not present in the cache
takes longer than accessing data inside of the cache. If what data
is being accessed depends on a secret value, the observed execution
time will thus also depend on this secret value. If an attacker is located
on the same machine they can place data into the cache as well, and probe
the cache themselves, to learn fine-grained information about the
program's access patterns. While seemingly far-fetched, this is easy
to achieve now that so many applications are run on cloud computing.

A wide variety of attacks involving caches have been mounted
against various cryptosystems
\cite{
  bernstein_cache-timing_2005,
  yarom_cachebleed_2017,
  cabrera_aldaya_cache-timing_2019}
, making accessing data based on secret
values fraught with peril.

\subsubsection{Our Threat-Model}
\label{threat_model}

Although the variety of potential timing side-channels is quite daunting,
we can distill them into a simple, albeit pessimistic set of rules:

\begin{enumerate}
  \item Any loop leaks the number of iterations taken.
  \item Any memory access leaks the address accessed.
  \begin{enumerate}
    \item As a consequence, accessing an array leaks the index accessed.
  \end{enumerate}
  \item Any conditional statement leaks which branch was taken.
\end{enumerate}

Rule 1 is justified by theoretical concerns, since a longer loop
requires more operations. In practice, it's difficult to observe
the iterations of each loop in a program from an overall timing signal,
making this a pessimistic rule.

Rule 2 is justified by various cache based side-channels and attacks
\cite{
  bernstein_cache-timing_2005,
  yarom_cachebleed_2017,
  cabrera_aldaya_cache-timing_2019}.
Since caches only load information an entire line at a time, it might
seem that our rule is too pessimistic, and that only which cache line
was accessed should be kept secret \cite{brickell_technologies_2011}.
Unfortunately, the potential for attacks on much finer grained
level has been demonstrated
\cite{
  bernstein_word_2013,
  osvik_cache_2006,
  yarom_cachebleed_2017}.
Because of these concerns, we take a pessimistic position, and assume
that accessing an array leaks the exact index accessed.


Rule 3 is justified in two ways. First, if different branches of a conditional
statement execute a different number of operations, this leaks information
about which branch was taken in a fundamental way. Second, even if both
branches execute identitical operations, the CPU's branch predictor
can be exploited, leaking information about which branch was taken
\cite{
  aciicmez_predicting_2006,
  aciicmez_power_2007,
  evtyushkin_jump_2016}.

In addition to these rules, we assume that addition, multiplication,
logical operations, and shifts, as implemented in hardware,
are constant-time in their inputs.
This is the case on most processors, one notable exception being
microprocessors
\cite{pornin_bearssl_nodate}. For the platforms which Go,
and thus our library, targets,
this assumption is reasonable.

\subsection{Vulnerabilities in \texttt{big.Int}}

Go provides a general purpose type for Big Numbers: \texttt{big.Int}.
This implementation is concerned with being broadly applicable,
and well-optimized. It does not focus on security, or on hardening
itself against timing side-channels.

Unfortunately, its broad applicability makes it useful for cryptography,
and it gets used throughout Go's standard cryptography library,
as we've seen previously.

In this section, we look at some of the important implementation aspects
of \texttt{big.Int}, and how they might be potentially vulnerable
according to our threat model.

\subsubsection{Padding}

The \texttt{big.Int} type always normalizes numbers internally,
removing any leading zero limbs. Even if you initialize a number
using bytes zero-padded to a certain length, the resulting value
will immediately chop off these zeros. These extra zeros don't
change the value of a number. By discarding them, operations on
this number will have fewer limbs to process, and will be faster.

Unfortunatelly, this means that \texttt{big.Int} leaks information
about the padding of numbers pervasively. Since every operation on
a number takes more time the more limbs the number uses,
the removal of padding leaks the true sizes of numbers
at every operation.
Leaking this padding has been exploited
in OpenSSL \cite{merget_raccoon_2019}, and might potentially
be a vulnerability in Go's cryptography library, because of \texttt{big.Int}.

\subsubsection{Leaky Algorithms}

Because \texttt{big.Int} is not written with Cryptography in mind,
its methods violate the rules set in
\ref{threat_model}. Many methods take a different number of iterations
based on their values, branch conditionally on values, and
access memory depending on values. Because \texttt{big.Int}
is designed for general purpose use, this problem should only get worse
as the library is further developed and optimized.

Ultimately, the problem is not the existence of \texttt{big.Int},
but it's use in Go's cryptography library, and in the broader ecosystem.

\subsubsection{Mitigations}

Although \texttt{big.Int} gets used in Go's cryptography library,
the authors are aware of its shortcomings, and have implemented
several mitigations to try and make its timing side-channels harder to
exploit.

One of the most important ones is a mitigation
for RSA: blinding \cite{kocher_timing_1996}. The decrypt a ciphertext
$c = m^e \mod N$, we normally calculate:

$$
c^d \mod N
$$

with $d$ our private key, and $(e, N)$ our public key.
When exponentiation is not implemented in a constant-time way, like
with \texttt{big.Int}, this process can leak information about $m$.
If an adversary can choose $c$, then this can leak information about
$d$ as well.

To mitigate this, instead of decrypting $c$ directly,
we first generate a random integer
$r \in [0, N - 1]$, and make sure
it has an inverse $r^{-1}$ mod $N$. Then we decrypt $r^e \cdot c$.
This gives us the value $r \cdot m$, and we can recover
$m$ by multiplying by $r^{-1}$.

While this does effectively mitigate the simplest attacks
against exponentiation, a very leaky operation, the other operations
involving these values are still left unprotected, and may
have exploitable leakages in more subtle ways. We also have
the unaddressed issue of padding, which has lead to attacks
in OpenSSL \cite{merget_raccoon_2019}.

\section{Implementation}

We've implemented a library, called
\texttt{safenum} \cite{meier_cronokirbysafenum_2021}, intended to provide
a replacement for \texttt{big.Int}, suitable for use in Cryptography.
In order to demonstrate its utility, we've replaced some
of \texttt{go/crypto}'s usage of \texttt{big.Int} with our own library,
in a separate repository
\cite{meier_cronokirbyctcrypto_2021}.

In this section, we go over the design and implementation of our
library.

\subsection{The \texttt{safenum} library}

Safenum defines a \texttt{Nat} type, which is intended to
replace \texttt{big.Int}. This type represents arbitrary
numbers in $\mathbb{N}$. Unlike \texttt{big.Int}, we do not handle
negative numbers. Handling a sign bit in constant-time is exceedingly
tricky. Thankfully, we haven't found this limitation to be restrictive
when replacing \texttt{big.Int} in Go's cryptography library.

We represent numbers in base $W := 2^{64}$. Concretely, we
store a number as a slice of type \texttt{[]uint}, in little
endian order. We call these the "limbs" of a number. For example,
the slice:
\begin{minted}{Go}
[]uint{13, 47, 52}
\end{minted}
represents the number:
$$
52 \cdot 2^{128} + 47 \cdot 2^{64} + 13
$$
These limbs might be padded, to conceal the true value of a number,
as we'll see later.

We provide operations for addition and multiplication of \texttt{Nats},
all other operations are for modular arithmetic. For modular arithmetic,
we provide numerous operations, including modular addition, subtraction,
multiplication, exponentiation, inversion, reduction, and taking
square roots modulo prime numbers. We also provide the usual operations
for serializing to and from bytes.

We try and structure the API in a similar way to \texttt{big.Int},
where an operation is performed on a buffer \texttt{Nat}, which
can receive the result. For example, this is the signature for
modular addition:

\begin{minted}{Go}
func (z *Nat) ModAdd(x *Nat, y *Nat, m *Modulus) *Nat
\end{minted}

This calculates $z \leftarrow x + y \mod m$, returning $z$. The advantage
of structuring the API this way, instead of simply returning a new
value, is that we can reuse the memory of $z$ for the result.

We go one step further, in fact, and use the memory of the buffer
\texttt{Nat} for all scratch space needed inside of an operation.
Structuring our operations this way allows us to limit unnecessary waste
of allocation.

\subsubsection{Handling Size}

Unlike \texttt{big.Int}, a \texttt{Nat} doesn't truncate its limbs
to remove any zero padding. Because of this, we distinguish
between the \emph{true size} of a number, which is how many significant
bits or limbs it actually has, and the \emph{announced size}
of a number, which is how many limbs are actually used to store
that number. The announced size is allowed to be leaked, while the
true size should be kept secret. The true size is always
at most the announced size

Because of this, we need to ensure that there's always a clear
announced size to use for the results that we produced. For modular
operations, we have an obvious choice: the size of the modulus.
When doing a modular operation, the result will always receive
the same announced size as the modulus does.

For example, when doing modular addition:

\begin{minted}{Go}
func (z *Nat) ModAdd(x *Nat, y *Nat, m *Modulus) *Nat
\end{minted}

our result \texttt{z} will have the same announced size as \texttt{m}.
After modular addition, we have that $z \in [0, m - 1]$ by definition.
Because of this, leaking the fact that the true size of $z$
is at most that of $m$ leaks no information about what value $z$
actually has, beyond what's knowable just by inspecting the call
graph in the source code of a program.

When serializing a \texttt{Nat}, we respect its announced size,
and produce zeros for padding as necessary. This is done without
any special handling, because we already store padded limbs anyways.

Similarly, when deserializing a \texttt{Nat} from bytes,
we respect any padding, unlike \texttt{big.Int}. For example,
if 32 big endian bytes are deserialized, we will end up
with a \texttt{Nat} with an announced size of 256 bits, regardless
of the value of those bytes.

This leaves us with non-modular addition and multiplication of numbers.
One approach is to use the maximum possible resulting size for our
result's announced length. For example, if we multiply
numbers $x_1$ and $x_2$, of announced size $b_1$ and $b_2$, then
our result will need a size of at most $b_1 + b_2$.
In situations where we know that our result will be smaller,
this size explosion can be undesirable.

Because of this, we opt towards letting users specify exactly how many
resulting bits they need in the output. For example,
multiplication has the following signature:

\begin{minted}{Go}
func (z *Nat) Mul(x *Nat, y *Nat, cap uint) *Nat
\end{minted}

Here \texttt{cap} is the number of bits that the result should have.
We use this to determine the result's announced length. Any output
beyond that capacity will simply be discarded.

In summary, the announced size of a \texttt{Nat} is always
clear based on how it's produced, and results from deserializing
a value, from using the same size as a modulus, or from manually
deciding on an output size.

\subsubsection{Moduli}

In our library, we've decided to make a separate type for representing
the moduli used in modular arithmetic: \texttt{Modulus}. There are several
reasons for doing this.

First, various operations in modular arithmetic require different properties
of the modulus which can be pre-computed. For example, montgomery multiplication
requires us to know $m^{-1} \mod W$, with $W$ our base,
and $m$ our modulus. By using a separate type for moduli, we can
precompute these values.

Second, the true size of a modulus is considered to be leakable.
As a consequence, moduli are stored \emph{without} padding.
This is desirable because modular reduction needs access to the most
significant bits of a modulus, and fetching this information without
leaking padding is exceedingly difficult. Furthermore,
by storing moduli without padding, the announced size of numbers produced
through modular operations is as tight as possible, which speeds
up operation.

This assumption is safe in cryptography. Moduli are often public,
like with the public modulus $N$ in RSA. In this case, leaking
the true size is fine, since even the exact value is known. There
are some cases where a secret modulus is necessary. For example,
when generating an RSA key, we use the factorization $N = pq$ of the modulus,
and calculate our private key modulo $\varphi(N) := (p - 1)(q - 1)$:

$$
d := e^{-1} \mod \varphi(N)
$$

Leaking the value of $\varphi(N)$ would be catastrophic. On the other hand,
it's clear that the true size of $\varphi(N)$ is approximately
that of $N$, which is known. In this case, leaking the true size of
$\varphi(N)$ is fine.

Using a separate modulus type is necessary to have this
weaker constraint on its announced size.

\subsection{Constant-Time Operations}

The rules we established in our threat model \ref{threat_model}
are quite stringent. For many operations, we want to have conditional
behavior depending on the values and results we see. Without access
to branching, this would seem impossible. Thankfully, there are workarounds
to enable us to have conditional behavior, all while not leaking information
about which conditions are selected. The core idea here is that whenever
we're faced with a choice, we perform both branches, and then combine
the results together without revealing which result we end up using.

For example, an standard algorithm for modular subtraction would look
like this (in pseudo-Go):

\begin{minted}{Go}
func (z *Nat) ModSub(x *Nat, y *Nat, m *Modulus) *Nat {
  borrow := z.Sub(x, y)
  if borrow == 1 {
    z.Add(z, m)
  }
}
\end{minted}

The problem here is that by conditionally adding in $m$, we reveal
whether or not $y > x$, and a borrow occurred. Our solution
requires a new primitive:

\begin{minted}{Go}
func (z *Nat) ctCondCopy(v choice, y *Nat) *Nat
\end{minted}

This function assigns $y$ to $z$ if $v = 1$, and does nothing otherwise.
Furthermore, this primitive shouldn't leak any information about
whether or not the condition was true.

With this in place, we can implement modular subtraction without leakage:

\begin{minted}{Go}
func (z *Nat) ModSub(x *Nat, y *Nat, m *Modulus) *Nat {
  borrow := z.Sub(x, y)
  scratch := new(Nat).Add(z, m)
  z.ctCondCopy(choice(borrow), scratch)
}
\end{minted}

We always perform the addition, and copy over the result if necessary,
without leaking the value of \texttt{borrow}.

This kind of rearrangement is the foundation that allows us to replace
branching in algorithms with constant-time operations.

\subsubsection{Building Primitives}

The question remains: how do you build up the primitives like
\texttt{ctCondCopy}, which let you choose results without
leaking which result was chosen?

The methods for constant-time choice are analagous to the standard
programming methods for conditional branching. Instead of using
\texttt{bool} to represent the result of a condition, we use

\begin{minted}{Go}
type choice Word
\end{minted}

The value of a choice is either $1$ or $0$, but we use the same
type as full limbs, to try and avoid having the compiler
turning or manipulations back into branches.

From this choice value, we can build a primitive that selects
between two limbs without leaking which choice was selected:

\begin{minted}{Go}
func ctIfElse(v choice, x, y Word) Word {
  mask := -Word(v)
  return y ^ (mask & (y ^ x))
} 
\end{minted}

This routine returns $x$ if $v = 1$, and $y$ otherwise. Unlike
a conditional statement, this is implemented through bitwise operations,
and doesn't leak information about what was selected.

If $v = 0$, then \texttt{mask} contains only zeros, and we're left
with \texttt{y}. When \texttt{v == 1},
then \texttt{mask} only contains ones. The \texttt{y}'s cancel eachother out,
leaving us with \texttt{x}.

We can use this primitive to build up a larger selection primitive,
allowing us to assign one slice to another, conditionally:

\begin{minted}{Go}
func ctCondCopy(v choice, x, y []Word) {
  for i := 0; i < len(x); i++ {
    x[i] = ctIfElse(v, y[i], x[i])
  }
}
\end{minted}

These primitives allow us to use \texttt{choice} to introduce conditional
behavior without leaking our choices, but we also need primitives to
create \texttt{choice} values in the first place. These are built up
in a similar way from small primitives to large primitives.

We can decide whether or not two limbs are equal using some bitwise
trickery:

\begin{minted}{Go}
func ctEq(x, y Word) choice {
  q := uint64(x ^ y)
  return 1 ^ choice((q|-q)>>63)
}
\end{minted}

To understand why this trick works, first realize that in two's complement,
either the most significant bit of a number is set, or the most significant
bit of a number's negation is set, unless that number is zero.
Thus, the expression
\begin{minted}{Go}
choice((q|-q)>>63)
\end{minted}
simply checks, using only bitwise operations, that
\texttt{q} is not zero.
Since \texttt{q} is zero precisely when \texttt{x} and \texttt{y}
are equal, we can simply negate the non-zero check.

We can use this primitive to compare entire slices of limbs
in constant time:

\begin{minted}{Go}
func cmpEq(x []Word, y []Word) choice {
  res := choice(1)
  for i := 0; i < len(x) && i < len(y); i++ {
    res &= ctEq(x[i], y[i])
  }
  return res
}
\end{minted}

We can't exit early, since that would leak information about
the value of x and y. Instead, we combine all the results together,
using the fact that two slices are equal when every one of their
matching limbs are equal.

\subsection{Algorithm Choices}

While going over how each operation works in detail is outside
the scope of this report, describing some of the high-level
techniques for certain trickier operations is nonetheless interesting.

Many of operations were inspired by the excellent
work of Thomas Pornin in BearSSL
\cite{pornin_bearssl_2020-1}.

\subsubsection{Modular Reduction}

To reduce a number $a$ modulo $m$, we first implement an operation
that allows us to shift in a single limb. This
lets us reduce a number
of the form:
$$
z := a \cdot W + b
$$
with $a \in [0, m - 1]$ and $b \in [0, W - 1]$. With this in place,
we can reduce an arbitrary number, by shifting in each of its limbs,
from most significant, to least significant, reducing modulo $m$
each time.

Implementing this shifting operation is a bit tricky. In the
case that $m$ has only a single limb, we can reduce
$z = z_1 W + z_0$ by dividing the 128 bit number $z1:z0$ by $m$,
and then using the remainder as our result.
We implement this division using bitwise operations, because
Go's equivalent \texttt{bits.Div} operation is not constant-time.

In the case that $m$ has more than one limb,
we want to estimate the quotient $q := \lfloor z / m \rfloor$,
and then calculate $z - q \cdot m$, to get our remainder.
A first estimate divides the most significant two limbs of $z$
with the most significant limb of $m$, using the division operation
we've already established. To improve this estimate,
we can take the most significant 64 bits of $m$, ignoring
the leading zero bits in $m$'s top limb, and align our 128 bits
of $z$ accordingly. It's because
of this need to skip past $m$'s leading zeros
that we'd like for moduli to not have padding.
This gives us an estimate $\hat{q} = q \pm 1$.
We can then calculate $z - \hat{q} \cdot m$, and then either
add or subtract $m$ based on the result. In practice,
this means always adding and subtracting $m$, and then mixing
in the right result in constant-time based on flags.

This technique is further described in \cite{pornin_bearssl_2020-1}.

\subsubsection{Multiplication}

\subsubsection{Exponentiation}

\subsubsection{Inversion}

\subsubsection{Even Inversion}

\subsubsection{Modular Square Roots}

\subsection{Implementation Techniques}

In this section, we describe a few remaining implementation choices
and techniques of interest.

\subsubsection{Saturated or Unsatured Limbs}

As mentioned previously, we store numbers in base
$W := 2^{64}$. This means that we use the full width of a register
to store each limb. Because of this, we say that are limbs
are \emph{saturated}. It's also possible to store
limbs \emph{unsaturated}, by using fewer than $64$ bits.
BearSSL 
\cite{pornin_bearssl_2020-1}
takes this approach, using only $31$ bits of the available $32$ bits
in the integers it uses. For $64$ bit registers, using $63$ bit unsaturated
limbs would be the option of choice.

There are two compelling reasons for using unsaturated limbs.

The first is that this leaves an extra bit of space to hold a carry
or borrow after an addition or subtraction, respectively. This allows
us to chain together carries to implement operations over multiple
limbs, without having to use assembly instructions. In Go,
this isn't really an issue, because \texttt{bits.Add} and
\texttt{bits.Sub} are provided to implement this intrinsics
in a cross-platform way.

The second comes that if we use $w$ bits for each limb, then montgomery
multiplication needs to work with a value of size $2w + 1$ bits. With a fully
saturated limb of $64$ bits, we need $129$ bits. This uses
an extra register compared to unsaturated limbs of $63$ bits. Because
montgomery multiplication is called very often during exponentiation,
this can yield considerable savings.

The disadvantage of using unsaturated limbs comes when converting
numbers to and from bytes. With fully saturated limbs, our
$64$ bit limbs are nicely composed of $8$ bytes. With $63$ bit limbs,
this isn't the case, making conversion to and from bytes more
complicated, and expensive. Using unsaturated limbs would
also require storing additional information about the exact
announced size of a number, instead of being able to use
the number of limbs directly.

Ultimately, we opted to use saturated limbs, in order to reuse
the assembly routines already implemented for low level
operations in \texttt{big.Int}. These were designed with
saturated limbs in mind, and thankfully, are constant-time.

\subsubsection{Redundant Reductions}

Our library is defined to prevent misuse. Because of this,
modular operations work even if their inputs are not already
reduced. For example, addition modulo $m$ should return the
right result, even if the inputs are greater than $m$.
Unfortunately, the cost of reducing inputs modulo $m$ when
they were already in range is not desirable, since this operation
is relatively expensive. Ideally, we'd like to avoid reducing
inputs when we already know that they're correctly reduced.

To implement this, each number stores a pointer to a modulus,
indicating that it is already reduced by this modulus.
When we reduce a number modulo $m$, we check this pointer,
and skip this reduction if it matches $m$. If we modify
the value of a number, we update the modulus it points to
accordingly, based on what method was called.
For example:
\begin{minted}{Go}
z.ModAdd(x, y, m)
\end{minted}
will set \texttt{z}'s modulus to \texttt{m}.

We only set this modulus pointer based on what methods are called,
never on the actual value of a result. Because of this, the dynamic
checks of this pointer only depend on the callgraph of our program.
Since this graph is statically determined, these redundant reduction
checks don't impact the constant-time properties of our library.

\section{Results}

We've compared the performance of our library
with \texttt{big.Int} operation by operation, as well as in the
context of the \texttt{go/crypto} package.
Overall, our library is about 2.6x slower than using
\texttt{big.Int} for most operations,
but only 2x slower in realistic situations.
In this section, we
present these results in detail.

\subsection{Comparison with \texttt{big.Int}}

We've set up a series of benchmarks to compare the performance
of \texttt{Nat} compared to \texttt{big.Int} on various operations.

The following operations are all implemented on values, exponents,
and moduli of 2048 bits. For raw addition and multiplication,
we use the full size necessary to represent the result in our benchmarks.

\begin{center}
 \begin{tabular}{|c | c | c | c|} 
 \hline
 Operation & op / s (\texttt{big.Int}) & op / s (\texttt{Nat}) & ratio \\ [0.5ex] 
 \hline\hline
 Addition & 7,811,989 & 10,622,287 & 0.74 \\
 \hline
 Multiplication & 1,282,808 & 434,802 & 2.95 \\
 \hline
 Modular Addition & 9,088,922 & 3,121,003 & 2.91 \\
 \hline
\end{tabular}
\end{center}

The most expensive operation, by far, is exponentiation. Because
of this, it's fair to compare the performance on these two types
mainly on this operation. We can see that \texttt{Nat} is 2.6x
compared to \texttt{big.Int} for exponentiation, although
varies in speed for other operations.

For comparing modular square roots, we used the primes
$p_3 = 2^{244} + 79$, which is $3 \mod 4$,
and $p_1 = 2^{244} + 153$ which is $1 \mod 4$. We use different primes
to test the various codepaths for modular square roots.

\subsection{Comparison with \texttt{go/crypto}}

We've created a forked package
\cite{meier_cronokirbyctcrypto_2021}
of \texttt{go/crypto}, where we've replaced the usage of \texttt{big.Int}
with our own \texttt{Nat} type for both RSA and DSA. All of the code
using \texttt{big.Int} has been replaced, with the exception of
primality checking. This endeavour demonstrates the utility of
our package for writing cryptographic code.

We've also run benchmarks to assess the performance impact, as we
show in the following table:

\begin{center}
 \begin{tabular}{|c | c | c | c|} 
 \hline
 Operation & op / s (\texttt{big.Int}) & op / s (\texttt{Nat}) & ratio \\ [0.5ex] 
 \hline\hline
 RSA Decrypt & 670 & 312 & 2.15 \\
 \hline
 RSA Sign & 675 & 372 & 1.81 \\
 \hline
 RSA Decrypt (3 Prime) & 1173 & 596 & 1.97 \\
 \hline
 DSA Sign & 6202 & 2625 & 2.36 \\
 \hline
 DSA Parameters & 0.89 & 1.64 & 0.54 \\
 \hline
\end{tabular}
\end{center}

We use a 2048 bit modulus for both RSA and DSA. For RSA, we use
the CRT optimization, instead of use exponentiation directly.
Our benchmarks for \texttt{big.Int} use blinding, but our benchmarks
for \texttt{Nat} do not. Because \texttt{Nat} is constant-time,
we don't need to use blinding to mitigate timing attacks.
We don't include DSA verification, since this can be safely done
with \texttt{big.Int}.

Overall, we can see that in a real world scenario, the use
of \texttt{Nat} is only 2x slower. This can surely be improved,
but is already an encouraging result.

\section{Further Work}

While we're happy with the utility of our library,
and the performance results we've managed to achieve, it's
of course still possible to improve on this front.

\subsection{Verifying Constant-Time Properties}

Ultimately, we would like to have more assurance about the
constant-time properties of our library. Our code hasn't
undergone an audit, nor have we verified the assembly output
produced by the Go compiler to ensure that it meets our demands.

Ideally, it would be nice to incorporate some kind of automated
analysis of our code to detect timing side-channels. An approach
similar to dudect
\cite{reparaz_dude_2017} 
might be an interesting way to provide a form of fuzz testing
to detect unwanted time-variation.

\subsection{Optimizing Assembly Routines}

Currently, we rely on some assembly routines pulled from
\texttt{big.Int}, slightly modified to avoid jumping to non-constant-time
routines. Unfortunately, not all of the primitive operations we would
like to have are present. Furthermore, we could reduce memory usage
in some places, by having this operations present a "conditional"
variant. For example, we could have an add operation taking a
\texttt{choice} flag, allowing us to choose whether or not to perform
an addition without leaking information. This would avoid having
to use a scratch buffer and a conditional copy.

To gain similar speed to the other primitives, these new primitives
would also need to be implemented in assembly. This would be time-consuming,
but likely worth the effort. There are also new solutions
to help with writing assembly routines in Go, such as the Avo library.

\subsection{Upstreaming to \texttt{go/crypto}}

While we believe our library is immediately useful for the broader
ecosystem, it's not realistically going to be replacing
the use of \texttt{big.Int} in Go's cryptography library any time soon.

The most likely path towards removing \texttt{big.Int} from
\texttt{go/crypto} is likely to move towards specialized arithmetic
implementations for each prime field involved in ECC. DSA is a legacy
algorithm, where the security flaws introduced
by \texttt{big.Int} are not of major concern.

This leaves RSA. Unfortunately, the nature of RSA, requiring dynamic
moduli, makes it so that a big number library of some kind is necessary.
Ideally, this library would be internal to RSA, allowing constant-time
operation, and severing the bridge between Go's cryptography
package, and \texttt{big.Int}.

As a proof of concept, we've implemented a fork of Go's RSA implementation,
replacing \texttt{big.Int} for encryption and decryption, 
with an internal number type, using the minimal amount of code
necessary to accomplish this.

Using unsaturated limbs, we've found that our fork of RSA
suffers only a 1.67x slowdown, while implementing encryption and
decryption in constant-time.

We hope to prepare a patch for Go's RSA implementation to merge
in this work soon.

\section{Conclusion}

In summary, we have shown
why Go's general purpose big number type, \texttt{big.Int},
is not suitable for Cryptography, for various reasons.
Unfortunately, this type gets used out of convenience,
even in Go's own cryptography library.

To address this, we've created a replacement library for \texttt{big.Int},
achieving a slowdown of only 2.6x for most operations, while attempting
to provide constant-time operation.

To test the utility of this library, we've replaced the usage
of \texttt{big.Int} in Go's implementation of RSA, and DSA, and found
only a slowdown of 2x.

\section*{Acknowledgements}

Firstly, I'd like to thank Prof. Bryan Ford for supervising this work.
I'd like to thank Pierluca Borsò, for letting me 
work on this project.
Finally, I'd also like to thank Daniel Huigens, and Marin Thiercelin,
from ProtonMail, for their advice and industry perspective on this work.

\addcontentsline{toc}{section}{Acknowledgements}

\bibliographystyle{plainurl}
\bibliography{references}
\end{document}
