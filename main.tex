\documentclass[11pt, a4paper]{article} %\documentclass[11pt, a4paper, twoside, openright]{article} %draft

\usepackage{graphicx,color}
\usepackage{amssymb, amsmath, array}
\usepackage{hyperref}
\usepackage{newpxtext, newpxmath}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{csquotes}
\usemintedstyle{friendly}
\usepackage[parfill]{parskip}

\begin{document}

\onecolumn

\input{cover}


\thispagestyle{empty}

\newpage

\tableofcontents{\protect\thispagestyle{empty}

\clearpage

\section{Introduction}

As more activity takes place digitally, the importance of
secure communication has never been greater.
Thankfully, after 50 years of Public Key cryptography
\cite{hellman_overview_1978},
we have good theoretical systems underpinning our security.

Most of these systems rely on modular arithmetic with large numbers.
For example, RSA \cite{rivest_method_1978},
or Elliptic Curve cryptography \cite{miller_use_1986}.
Working with these numbers is not natively supported by hardware.
Instead, we use a \enquote{Big Number}
software library to provide this functionality.

Unfortunately, although Public Key cryptosystems have been heavily
scrutinized in theory,
implementations often suffer from vulnerabilities in practice

One important class of vulnerability are
timing side-channels
\cite{kocher_cryptanalysis_1995}.
This is where an implementation leaks information
about secret values through its execution time.
Big Numbers libraries designed without cryptography in mind
suffer from these vulnerabilities.

In particular, Go \cite{the_go_authors_go_nodate} provides a general purpose
Big Number type, \texttt{big.Int},
which does not provide constant-time operations.
Unfortunately, this library
gets used for cryptography
\cite{ford_proposal_2017}, including inside of Go's own standard library,
in the \texttt{go/crypto} package.

We've addressed this issue by creating a library
\cite{meier_cronokirbysafenum_2021}
providing Big Numbers with constant-time operations.
Our library provides the necessary operations for Public Key cryptography,
all while avoiding leakage through timing side-channels.
To demonstrate its utility, we've modified Go's \texttt{go/crypto}
package, replacing the use of \texttt{big.Int} in the DSA and RSA
systems, achieving a slowdown of only 2x in this scenario.

\section{Background}

In this section, we explain how Big Numbers are used in Public Key
cryptography, what timing side-channels are, how we model their threat,
as well as what kind of vulnerabilities are present in Go's
\texttt{big.Int} type.

\subsection{Big Numbers in Cryptography}

Most Public Key cryptosystems rely on
modular arithmetic.

In RSA \cite{rivest_method_1978}, for example, a public key $(e, N)$ consists
of a modulus
$N \in \mathbb{N}$, usually 2048 bits long, and an exponent $e
\in [0, \varphi(N) - 1]$.
To encrypt a message $m \in [0, N - 1]$, we calculate
$$
c := m^e \mod N
$$

Since $N$ is much larger than a register, 
we need a Big Number library to implement this system.
Modular exponentiation is also not supported in hardware,
requiring extra support in software.

DSA
\cite{technology_digital_1994} also relies on modular arithmetic,
this time using a large prime $p$ of around 2048 bits, and working
in the multiplicative group $(\mathbb{Z}/p\mathbb{Z})^*$.

Elliptic Curve Cryptography
\cite{miller_use_1986} relies on complex formulas for adding points
on an elliptic curve, built over a finite field $K$. This field
is usually either a prime field $\mathbb{Z}/p\mathbb{Z}$, in which case arithmetic
modulo $p$ is used, or a
binary extension field $\text{GF}(2^n)$, in which case
binary and polynomial arithmetic are used.
For prime fields, a Big Number library necessary, because
the size of the prime is greater than 200 bits.

\subsubsection{Implementing Big Numbers}

When a modulus is known in advance, a special purpose
library implementing arithmetic with this fixed modulus
can be used. This is the case for Elliptic Curve cryptography,
where the prime field is a fixed parameter of the system.
Using a fixed type makes it easier to provide constant-time operation.

A disadvantage is that since different systems require different moduli,
it takes more work to implement and support each system.
One way to address this is to automatically generate implementations
of modular arithmetic, as done by FiatCrypto
\cite{hvass_high-assurance_nodate}.

In some systems, you need support for dynamic moduli.
Take RSA, for example.
In this case, you need a library supporting dynamically
sized numbers, as well as various operations.

\subsubsection{Big Numbers in \texttt{go/crypto}}

Go provides implementations of the Public Key cryptosystems
we've mentioned so far in its \texttt{go/crypto} package.

Unfortunately \cite{ford_proposal_2017}, the general purpose
\texttt{big.Int} is used, in part, to implement these systems,
despite its potential vulnerability to timing attacks.

For DSA \cite{technology_digital_1994}, Go uses \texttt{big.Int}
for all operations, including key generation, signing, and verification.

For RSA \cite{rivest_method_1978}, Go embeds \texttt{big.Int}
as part of the API for
this package. Key generation, encryption, decryption, signing,
and verification all use \texttt{big.Int}.

For ECC \cite{miller_use_1986}, Go defines a general interface
for Elliptic Curves, requiring operations like point addition,
scalar multiplication, etc. All of these are defined in terms
of \texttt{big.Int}. Some of the curves have specialized
implementations of their prime field, only converting from
\texttt{big.Int} to satisfy this interface. The remaining curves
directly use \texttt{big.Int} for their field arithmetic.

\subsection{Timing Attacks}

A side-channel
\cite{kelsey_side_1998}
leaks secret information indirectly,
through the observable effects of a program's execution.
For example, timing side-channels use the execution time of a program
to gain information about the secret data it handles. 
A timing attack is the use of a timing side-channel to break the security
of some program or cryptographic algorithm.

When a program takes a different number of steps based on the value of some
secret, this is an obvious timing side-channel.
For example, if a naive program for comparing inputs with a secret password
stops as soon as a mismatch is found, then the algorithm itself has
a timing side-channel. This side-channel can be exploited, to allow
the secret password to be guessed byte-by-byte.

Not all timing side-channels are this simple. Algorithms
that always take the same number of steps
can still having timing side-channels because of the underlying
hardware. For example, a processor may
execute an operation faster for some inputs, or the presence of a cache
could be used to infer what addresses are being accessed. These
microarchitectural timing side-channels are also of concern.
See \cite{ge_survey_2018} for a survey of these vulnerabilities.

\subsubsection{Actual Attacks}

Although the presence of a side-channel
does not directly lead to attacks
as early as 1995, Paul Kocher
demonstrated the potential for timing side-channels to break
crypto-systems
\cite{kocher_cryptanalysis_1995, kocher_timing_1996}.
These attacks relied on algorithms that perform a varying number of
operations based on secret data. 

One objection to timing attacks is that
while a timing side-channel is catastrophic in theory, in practice
this channel is too noisy to exploit. Unfortunately,
it's possible to exploit these attacks, even across
a network \cite{brumley_remote_2005, brumley_remote_2011}.
Noise only makes the channel more difficult to exploit,
requiring more samples to detect the underlying signal.

The use of caches as a potential side-channel was
identified early on as well \cite{page_theoretical_2002}.
Accessing data takes longer when that data is outside
of the cache.
If data accesses depend on a secret value, the observed execution
time will also depend on this value. Additionally,
an attacker located on the same machine
can place data into the cache, and probe
the cache themselves, to learn even finer information about the
program's access patterns. This kind of colocation increasingly common,
as more applications are run on cloud servers.

A wide variety of attacks involving caches have been mounted
against various cryptosystems
\cite{
  bernstein_cache-timing_2005,
  yarom_cachebleed_2017,
  cabrera_aldaya_cache-timing_2019}
. Accessing data is thus based on secret
values fraught with peril.

\subsubsection{Our Threat-Model}
\label{threat_model}

We can distill this variety of side-channels into a simple,
albeit pessimistic, set of rules:

\begin{enumerate}
  \item Any loop leaks the number of iterations taken.
  \item Any memory access leaks the address accessed.
  \begin{enumerate}
    \item As a consequence, accessing an array leaks the index accessed.
  \end{enumerate}
  \item Any conditional statement leaks which branch was taken.
\end{enumerate}

Rule 1 is justified by theoretical concerns: a longer loop
uses more operations. In practice, it's difficult to observe
the iterations of each loop in a program from an overall timing signal,
making this a pessimistic rule.

Rule 2 is justified by various cache based side-channels and attacks
\cite{
  bernstein_cache-timing_2005,
  yarom_cachebleed_2017,
  cabrera_aldaya_cache-timing_2019}.
Since caches only load information an entire line at a time, it might
seem that our rule is too pessimistic, and that only which cache line
was accessed should be kept secret \cite{brickell_technologies_2011}.
Unfortunately, it's possible to perform
attacks on a much finer level
\cite{
  bernstein_word_2013,
  osvik_cache_2006,
  yarom_cachebleed_2017}.
This is why we take a pessimistic position, and assume
that accesses leak their exact address.


Rule 3 is justified in two ways. First, if different branches of a conditional
statement execute a different number of operations,
we can easily observe which branch was taken. Second, even if both
branches execute identitical operations, the CPU's branch predictor
can be exploited to leak information about which branch was taken
\cite{
  aciicmez_predicting_2006,
  aciicmez_power_2007,
  evtyushkin_jump_2016}.

In addition to these rules, we assume that addition, multiplication,
logical operations, and shifts, as implemented in hardware,
are constant-time in their inputs.
This is the case on most processors, one notable exception being
microprocessors
\cite{pornin_bearssl_nodate}. This assumption is reasonable
for the platforms targeted by Go and our library.

\subsection{Vulnerabilities in \texttt{big.Int}}

Go provides a general purpose type for Big Numbers: \texttt{big.Int}.
This type focuses on being optimized, and useful in various
situations. It does not focus on security, or on protecting
against timing side-channels.
Unfortunately, out of convenience, it gets used in
cryptography, even inside of Go's standard library.

In this section, we look at some of the important implementation aspects
of \texttt{big.Int}, and how they might be potentially vulnerable
according to our threat model.

\subsubsection{Padding}

The \texttt{big.Int} type normalizes numbers internally,
removing any leading zero limbs. Even if you initialize a number
using bytes zero-padded to a certain length, the resulting value
will immediately chop off these zeros.
By discarding them, operations on
this number will have fewer limbs to process, and will be faster.

Unfortunately, this means that \texttt{big.Int} pervasively leaks information
about the padding of numbers. Since every operation on
a number takes more time the more limbs the number uses,
every operation leaks the padding of numbers.
This has been exploited
in OpenSSL \cite{merget_raccoon_2019}, and might potentially
be a vulnerability in Go's cryptography library.

\subsubsection{Leaky Algorithms}

Because \texttt{big.Int} is not written with cryptography in mind,
its operations violate the rules in
\ref{threat_model}. Many methods take a different number of iterations
, branch conditionally, or
access memory differently
depending on their values. Because \texttt{big.Int}
is designed for general purpose use, this problem should only get worse
as the library is further developed and optimized.

Ultimately, the problem is not the existence of \texttt{big.Int},
but its use in Go's cryptography library, and in the broader ecosystem.

\subsubsection{Mitigations}

Although \texttt{big.Int} gets used in Go's cryptography library,
the authors are aware of its shortcomings, and have implemented
several mitigations to try and make its timing side-channels harder to
exploit.

One of the most important ones is a mitigation
for RSA: blinding \cite{kocher_timing_1996}.

To decrypt a ciphertext
$c = m^e \mod N$, we would normally calculate:

$$
c^d \mod N
$$

with $d$ our private key, and $(e, N)$ our public key.
When exponentiation is not implemented in a constant-time way, like
with \texttt{big.Int}, this process can leak information about $m$.
If an adversary can choose $c$, then this can leak information about
$d$ as well.

To mitigate this, instead of decrypting $c$ directly,
we first generate a random integer
$r \in (\mathbb{Z}/N\mathbb{Z})^*$. Then, we decrypt $r^e \cdot c$.
This gives us the value $r \cdot m$, and we can recover
$m$ by multiplying by $r^{-1}$.

While this effectively mitigates the simplest attacks
against exponentiation, a very leaky operation,
other methods are left unprotected, and may
have subtle exploits. We also have
the unaddressed issue of padding, which has lead to attacks
in OpenSSL \cite{merget_raccoon_2019}.

\section{Implementation}

We've implemented a library, called
\texttt{safenum} \cite{meier_cronokirbysafenum_2021}, intended to provide
a replacement for \texttt{big.Int}, suitable for use in cryptography.
In order to demonstrate its utility, we've replaced some
of \texttt{go/crypto}'s usage of \texttt{big.Int} with our own library,
in a separate repository
\cite{meier_cronokirbyctcrypto_2021}.

In this section, we go over the design and implementation of our
library.

\subsection{The \texttt{safenum} library}

Safenum defines a \texttt{Nat} type, intended to
replace \texttt{big.Int}. This type represents arbitrary
numbers in $\mathbb{N}$. Unlike \texttt{big.Int}, we do not handle
negative numbers. Handling a sign bit in constant-time is exceedingly
tricky. Thankfully, we haven't found this limitation to be restrictive
when replacing \texttt{big.Int} in Go's cryptography library.

We represent numbers in base $W := 2^{64}$. Concretely, we
store a number as a slice of type \texttt{[]uint}, in little
endian order. We call these the \enquote{limbs} of a number. For example,
the slice:
\begin{minted}{Go}
[]uint{13, 47, 52}
\end{minted}
represents the number:
$$
52 \cdot 2^{128} + 47 \cdot 2^{64} + 13
$$
These limbs might be padded, to conceal the true value of a number,
as we'll see later.

We provide operations for addition and multiplication of \texttt{Nats}.
We also provide numerous operations
for modular arithmetic, including modular addition, subtraction,
multiplication, exponentiation, inversion, reduction, and taking
square roots modulo prime numbers. We also provide the usual operations
for serializing to and from bytes.

We try and structure the API in a similar way to \texttt{big.Int},
where an operation is performed on a separate
\texttt{Nat} to receive the result.
For example, this is the signature for
modular addition:

\begin{minted}{Go}
func (z *Nat) ModAdd(x *Nat, y *Nat, m *Modulus) *Nat
\end{minted}

This calculates $z \leftarrow x + y \mod m$, returning $z$. The advantage
of structuring the API this way, instead of simply returning a new
value, is that we can reuse the memory of $z$ for the result.

We go one step further, in fact, and use the memory of the buffer
\texttt{Nat} for all scratch space needed inside of an operation.
Structuring our operations this way allows us to limit memory waste.

\subsubsection{Handling Size}

Unlike \texttt{big.Int}, a \texttt{Nat} doesn't truncate its limbs
to remove any zero padding. Because of this, we distinguish
between the \emph{true size} of a number, which is how many significant
bits or limbs it actually has, and the \emph{announced size}
of a number, which is how many limbs are actually used to store
that number. The announced size is allowed to be leaked, while the
true size should be kept secret. The true size is always
at most the announced size

Because of this, we need to ensure that there's always a clear
announced size to use for the results that we produce. For modular
operations, we have an obvious choice: the size of the modulus.
When doing a modular operation, the result will always receive
the same announced size as the modulus does.

For example, when doing modular addition:

\begin{minted}{Go}
func (z *Nat) ModAdd(x *Nat, y *Nat, m *Modulus) *Nat
\end{minted}

our result \texttt{z} will have the same announced size as \texttt{m}.
After modular addition, we have that $z \in [0, m - 1]$ by definition.
Because of this, leaking the fact that the true size of $z$
is at most that of $m$ leaks no information about what value $z$
actually has, beyond what's knowable just by inspecting the call
graph in the source code of a program.

When serializing a \texttt{Nat}, we respect its announced size,
and produce zeros for padding as necessary. This is done without
any special handling, because we already store padded limbs anyways.

Similarly, when deserializing a \texttt{Nat} from bytes,
we respect any padding, unlike \texttt{big.Int}. For example,
if 32 big endian bytes are deserialized, we will end up
with a \texttt{Nat} with an announced size of 256 bits, regardless
of the value of those bytes.

This leaves us with non-modular addition and multiplication of numbers.
One approach is to use the maximum possible resulting size for our
result's announced length. For example, if we multiply
numbers $x_1$ and $x_2$, of announced size $b_1$ and $b_2$, then
our result will need a size of at most $b_1 + b_2$.
In situations where we know that our result will be smaller,
this size explosion can be undesirable.

Because of this, we opt towards letting users specify exactly how many
resulting bits they need in the output. For example,
multiplication has the following signature:

\begin{minted}{Go}
func (z *Nat) Mul(x *Nat, y *Nat, cap uint) *Nat
\end{minted}

Here \texttt{cap} is the number of bits that the result should have.
We use this to determine the result's announced length. Any output
beyond that capacity will simply be discarded.

In summary, the announced size of a \texttt{Nat} is always
clear based on how it's produced, and results from deserializing
a value, from using the same size as a modulus, or from manually
deciding on an output size.

\subsubsection{Moduli}

In our library, we've decided to make a separate type for representing
the moduli used in modular arithmetic: \texttt{Modulus}. There are several
reasons for doing this.

First, various operations in modular arithmetic require different properties
of the modulus which can be pre-computed. For example, montgomery multiplication
requires us to know $m^{-1} \mod W$, with $W$ our base,
and $m$ our modulus. By using a separate type for moduli, we can
precompute these values.

Second, the true size of a modulus is considered to be leakable.
As a consequence, moduli are stored \emph{without} padding.
This is desirable because modular reduction needs access to the most
significant bits of a modulus, and fetching this information without
leaking padding is exceedingly difficult. Furthermore,
by storing moduli without padding, the announced size of numbers produced
through modular operations is as tight as possible, which speeds
up operation.

This assumption is safe in cryptography. Moduli are often public,
like with the public modulus $N$ in RSA. In this case, leaking
the true size is fine, since even the exact value is known. There
are some cases where a secret modulus is necessary. For example,
when generating an RSA key, we use the factorization $N = pq$ of the modulus,
and calculate our private key modulo $\varphi(N) = (p - 1)(q - 1)$:

$$
d := e^{-1} \mod \varphi(N)
$$

Leaking the value of $\varphi(N)$ would be catastrophic. On the other hand,
it's clear that the true size of $\varphi(N)$ is approximately
that of $N$, which is known. In this case, leaking the true size of
$\varphi(N)$ is fine.

Using a separate modulus type is necessary to have this
weaker constraint on its announced size.

\subsection{Constant-Time Operations}

The rules we established in our threat model \ref{threat_model}
are quite stringent. For many operations, we want to have conditional
behavior depending on the values and results we see. Without access
to branching, this would seem impossible. Thankfully, there are workarounds
to enable us to have conditional behavior, all while not leaking information
about which conditions are selected. The core idea here is that whenever
we're faced with a choice, we perform both branches, and then combine
the results together without revealing which result we end up using.

For example, a standard algorithm for modular subtraction would look
like this (in pseudo-Go):

\begin{minted}{Go}
func (z *Nat) ModSub(x *Nat, y *Nat, m *Modulus) *Nat {
  borrow := z.Sub(x, y)
  if borrow == 1 {
    z.Add(z, m)
  }
}
\end{minted}

The problem here is that by conditionally adding in $m$, we reveal
whether or not $y > x$, and a borrow occurred. Our solution
requires a new primitive:

\begin{minted}{Go}
func (z *Nat) ctCondCopy(v choice, y *Nat) *Nat
\end{minted}

This function assigns $y$ to $z$ if $v = 1$, and does nothing otherwise.
Furthermore, this primitive shouldn't leak any information about
whether or not the condition was true.

With this in place, we can implement modular subtraction without leakage:

\begin{minted}{Go}
func (z *Nat) ModSub(x *Nat, y *Nat, m *Modulus) *Nat {
  borrow := z.Sub(x, y)
  scratch := new(Nat).Add(z, m)
  z.ctCondCopy(choice(borrow), scratch)
}
\end{minted}

We always perform the addition, and copy over the result if necessary,
without leaking the value of \texttt{borrow}.

This kind of rearrangement is the foundation that allows us to replace
branching in algorithms with constant-time operations.

\subsubsection{Building Primitives}

The question remains: how do you build up the primitives like
\texttt{ctCondCopy}, which let you choose results without
leaking which result was chosen?

The methods for constant-time choice are analagous to the standard
programming methods for conditional branching. Instead of using
\texttt{bool} to represent the result of a condition, we use

\begin{minted}{Go}
type choice Word
\end{minted}

The value of a choice is either $1$ or $0$, but we use the same
type as full limbs, to try and avoid having the compiler
turning or manipulations back into branches.

From this choice value, we can build a primitive that selects
between two limbs without leaking which choice was selected:

\begin{minted}{Go}
func ctIfElse(v choice, x, y Word) Word {
  mask := -Word(v)
  return y ^ (mask & (y ^ x))
} 
\end{minted}

This routine returns $x$ if $v = 1$, and $y$ otherwise. Unlike
a conditional statement, this is implemented through bitwise operations,
and doesn't leak information about what was selected.

If $v = 0$, then \texttt{mask} contains only zeros, and we're left
with \texttt{y}. When \texttt{v == 1},
then \texttt{mask} only contains ones. The \texttt{y}'s cancel eachother out,
leaving us with \texttt{x}.

We can use this primitive to build up a larger selection primitive,
allowing us to assign one slice to another, conditionally:

\begin{minted}{Go}
func ctCondCopy(v choice, x, y []Word) {
  for i := 0; i < len(x); i++ {
    x[i] = ctIfElse(v, y[i], x[i])
  }
}
\end{minted}

These primitives allow us to use \texttt{choice} to introduce conditional
behavior without leaking our choices, but we also need primitives to
create \texttt{choice} values in the first place. These are built up
in a similar way from small primitives to large primitives.

We can decide whether or not two limbs are equal using some bitwise
trickery:

\begin{minted}{Go}
func ctEq(x, y Word) choice {
  q := uint64(x ^ y)
  return 1 ^ choice((q|-q)>>63)
}
\end{minted}

To understand why this trick works, first realize that in two's complement,
either the most significant bit of a number is set, or the most significant
bit of a number's negation is set, unless that number is zero.
Thus, the expression
\begin{minted}{Go}
choice((q|-q)>>63)
\end{minted}
simply checks, using only bitwise operations, that
\texttt{q} is not zero.
Since \texttt{q} is zero precisely when \texttt{x} and \texttt{y}
are equal, we can simply negate the non-zero check.

We can use this primitive to compare entire slices of limbs
in constant time:

\begin{minted}{Go}
func cmpEq(x []Word, y []Word) choice {
  res := choice(1)
  for i := 0; i < len(x) && i < len(y); i++ {
    res &= ctEq(x[i], y[i])
  }
  return res
}
\end{minted}

We can't exit early, since that would leak information about
the value of x and y. Instead, we combine all the results together,
using the fact that two slices are equal when every one of their
matching limbs are equal.

\subsection{Algorithm Choices}

While going over how each operation works in detail is outside
the scope of this report, describing some of the high-level
techniques for certain trickier operations is nonetheless interesting.

Many of these operations were inspired by the excellent
work of Thomas Pornin in BearSSL
\cite{pornin_bearssl_2020-1}.

\subsubsection{Modular Reduction}

To reduce a number $a$ modulo $m$, we first implement an operation
that allows us to shift in a single limb. This
lets us reduce a number
of the form:
$$
z := a \cdot W + b
$$
with $a \in [0, m - 1]$ and $b \in [0, W - 1]$. With this in place,
we can reduce an arbitrary number, by shifting in each of its limbs,
from most significant, to least significant, reducing modulo $m$
each time.

Implementing the shifting operation involves estimating
the quotient $q := \lfloor z / m \rfloor$, by dividing
the most significant 64 bits of $m$ with
the corresponding 128 bits of $z$,
and then
calculating $z - q m$, potentially fixing the result
by adding or subtracting $m$ again.
This technique is further described in \cite{pornin_bearssl_2020-1}.


\subsubsection{Exponentiation}

For exponentiation, we use left-to-right exponentiation,
with a window size of 4 bits. In order to do window lookups
in constant-time, we do a constant-time conditional assignment
over each of the possible operands. Even though this requires traversing
the entire window table every time, we've found that this method
is still faster than using a window size of 2 bits.

\subsubsection{Multiplication}

We implement Montgomery Multiplication
\cite{kaya_koc_analyzing_1996-1, pornin_bearssl_2020-1}, both
for exponentiation, but also for standard modular multiplication.

To calculate $ab \mod m$,
where $m$ has $n$ limbs, we first calculate the montgomery representation
$aW^n \mod m$, using the modular shift operation we implemented for
reduction. Then, we calculate the montgomery product
between $aW^n$ and $b$. We've found this method to be faster
than multliplying $a$ and $b$ to produce a $2n$ limb result, and then
calculating a modular reduction.

\subsubsection{Inversion}

For modular inversion, we use a variant of
the binary GCD algorithm, as described in \cite{pornin_optimized_2020}.
We have yet to implement the most optimized version, which accumulates
intermediate results into single limb registers, and instead
perform full width operations at each iteration.

\subsubsection{Even Inversion}

To calculate $x^{-1} \mod m$, when $m$ is an even integer,
we employ a standard trick. First, we calculate
$u := m^{-1} \mod x$, and then we calculate our desired inverse as:

$$
\frac{um - 1}{x}
$$

\subsubsection{Modular Square Roots}

To calculate $\sqrt{z} \mod p$, we assume that $p$ is prime.
Which algorithm we use depends on whether $p = 3 \mod 4$,
or $p = 1 \mod 4$.

In the first case, we can calculate a root as:
$$
\sqrt{z} = z^{(p + 1) / 4} \mod p
$$

In the second case, we use a constant-time variant
of the Tonnelli-Shanks algorithm, as described in 
\cite{wahby_hashing_nodate}.

\subsection{Implementation Techniques}

In this section, we describe a few remaining implementation choices
and techniques of interest.

\subsubsection{Saturated or Unsatured Limbs}

As mentioned previously, we store numbers in base
$W := 2^{64}$. This means that we use the full width of a register
to store each limb. Because of this, we say that are limbs
are \emph{saturated}. It's also possible to store
limbs \emph{unsaturated}, by using fewer than $64$ bits.
BearSSL 
\cite{pornin_bearssl_2020-1}
takes this approach, using only $31$ bits of the available $32$ bits
in the integers it uses. For $64$ bit registers, using $63$ bit unsaturated
limbs would be the option of choice.

There are two compelling reasons for using unsaturated limbs.

The first is that this leaves an extra bit of space to hold a carry
or borrow after an addition or subtraction, respectively. This allows
us to chain together carries to implement operations over multiple
limbs, without having to use assembly instructions. In Go,
this isn't really an issue, because \texttt{bits.Add} and
\texttt{bits.Sub} are provided to implement these intrinsics
in a cross-platform way.

The second comes that if we use $w$ bits for each limb, then montgomery
multiplication needs to work with a value of size $2w + 1$ bits. With a fully
saturated limb of $64$ bits, we need $129$ bits. This uses
an extra register compared to unsaturated limbs of $63$ bits. Because
montgomery multiplication is called very often during exponentiation,
this can yield considerable savings.

The disadvantage of using unsaturated limbs comes when converting
numbers to and from bytes. With fully saturated limbs, our
$64$ bit limbs are nicely composed of $8$ bytes. With $63$ bit limbs,
this isn't the case, making conversion to and from bytes more
complicated, and expensive. Using unsaturated limbs would
also require storing additional information about the exact
announced size of a number, instead of being able to use
the number of limbs directly.

Ultimately, we opted to use saturated limbs, in order to reuse
the assembly routines already implemented for low level
operations in \texttt{big.Int}. These were designed with
saturated limbs in mind, and thankfully, are constant-time.

\subsubsection{Redundant Reductions}

Our library is defined to prevent misuse. Because of this,
modular operations work even if their inputs are not already
reduced. For example, addition modulo $m$ should return the
right result, even if the inputs are greater than $m$.
Unfortunately, the cost of reducing inputs modulo $m$ when
they were already in range is not desirable, since this operation
is relatively expensive. Ideally, we'd like to avoid reducing
inputs when we already know that they're correctly reduced.

To implement this, each number stores a pointer to a modulus,
indicating that it is already reduced by this modulus.
When we reduce a number modulo $m$, we check this pointer,
and skip this reduction if it matches $m$. If we modify
the value of a number, we update the modulus it points to
accordingly, based on what method was called.
For example:
\begin{minted}{Go}
z.ModAdd(x, y, m)
\end{minted}
will set \texttt{z}'s modulus to \texttt{m}.

We only set this modulus pointer based on what methods are called,
never on the actual value of a result. Because of this, the dynamic
checks of this pointer only depend on the callgraph of our program.
Since this graph is statically determined, these redundant reduction
checks don't impact the constant-time properties of our library.

\section{Results}

We've compared the performance of our library
with \texttt{big.Int} operation by operation, as well as in the
context of the \texttt{go/crypto} package.
Overall, our library is about 2.6x slower than using
\texttt{big.Int} for most operations,
but only 2x slower in realistic situations.
In this section, we
present these results in detail.

\subsection{Comparison with \texttt{big.Int}}

We've set up a series of benchmarks to compare the performance
of \texttt{Nat} compared to \texttt{big.Int} on various operations.

The following operations are all implemented on values, exponents,
and moduli of 2048 bits. For raw addition and multiplication,
we use the full size necessary to represent the result in our benchmarks.

\begin{center}
 \begin{tabular}{|c | c | c | c|} 
 \hline
 Operation & op / s (\texttt{big.Int}) & op / s (\texttt{Nat}) & ratio \\ [0.5ex] 
 \hline\hline
 Addition & 10,980,842 & 12,164,599 & 0.90 \\
 \hline
 Modular Addition & 6,986,739 & 3,075,188 & 2.27 \\
 \hline
 Multiplication & 1,316,322 & 542,385 & 2.43 \\
 \hline
 Modular Reduction & 454,917 & 63,253 & 7.19 \\
 \hline
 Modular Multiplication & 1,000,000 & 44,596 & 22.42 \\
 \hline
 Modular Inversion & 1,000,000 & 621 & 1610 \\
 \hline
 Modular Exponentiation & 223 & 86 & 2.59 \\
 \hline
\end{tabular}
\end{center}

The most expensive operation, by far, is exponentiation. Because
of this, it's fair to compare the performance on these two types
mainly on this operation. We can see that \texttt{Nat} is 2.6x slower
compared to \texttt{big.Int} for exponentiation, although
some operations are much slower.

For comparing modular square roots, we used the primes
$p_3 = 2^{244} + 79$, which is $3 \mod 4$,
and $p_1 = 2^{244} + 153$ which is $1 \mod 4$. We use different primes
to test the various codepaths for modular square roots:

\begin{center}
 \begin{tabular}{|c | c | c | c|} 
 \hline
 Operation & op / s (\texttt{big.Int}) & op / s (\texttt{Nat}) & ratio \\ [0.5ex] 
 \hline\hline
 $\sqrt{z} \mod p_3$ & 40,464 & 26,886 & 1.50 \\
 \hline
 $\sqrt{z} \mod p_1$ & - & 7,867 & - \\
 \hline
\end{tabular}
\end{center}

We didn't manage to find a large value where Go's Tonneli Shanks routine
managed to find a square root without hanging, although we
expect the ratio to be similar to the other case.

\subsection{Comparison with \texttt{go/crypto}}

We've created a forked package
\cite{meier_cronokirbyctcrypto_2021}
of \texttt{go/crypto}, where we've replaced the usage of \texttt{big.Int}
with our own \texttt{Nat} type for both RSA and DSA. All of the code
using \texttt{big.Int} has been replaced, with the exception of
primality checking. This endeavour demonstrates the utility of
our package for writing cryptographic code.

We've also run benchmarks to assess the performance impact, as we
show in the following table:

\begin{center}
 \begin{tabular}{|c | c | c | c|} 
 \hline
 Operation & op / s (\texttt{big.Int}) & op / s (\texttt{Nat}) & ratio \\ [0.5ex] 
 \hline\hline
 RSA Decrypt & 670 & 312 & 2.15 \\
 \hline
 RSA Sign & 675 & 372 & 1.81 \\
 \hline
 RSA Decrypt (3 Prime) & 1173 & 596 & 1.97 \\
 \hline
 DSA Sign & 6202 & 2625 & 2.36 \\
 \hline
 DSA Parameters & 0.89 & 1.64 & 0.54 \\
 \hline
\end{tabular}
\end{center}

We use a 2048 bit modulus for both RSA and DSA. For RSA, we use
the CRT optimization, instead of using exponentiation directly.
Our benchmarks for \texttt{big.Int} use blinding, but our benchmarks
for \texttt{Nat} do not. Because \texttt{Nat} is constant-time,
we don't need to use blinding to mitigate timing attacks.
We don't include DSA verification, since this can be safely done
with \texttt{big.Int}.

Overall, we can see that in a real world scenario, the use
of \texttt{Nat} is only 2x slower. This can surely be improved,
but is already an encouraging result.

\section{Further Work}

While we're happy with the utility of our library,
and the performance results we've managed to achieve, it's
of course still possible to improve on this front.

\subsection{Verifying Constant-Time Properties}

Ultimately, we would like to have more assurance about the
constant-time properties of our library. Our code hasn't
undergone an audit, nor have we verified the assembly output
produced by the Go compiler to ensure that it meets our demands.

Ideally, it would be nice to incorporate some kind of automated
analysis of our code to detect timing side-channels. An approach
similar to dudect
\cite{reparaz_dude_2017} 
might be an interesting way to provide a form of fuzz testing
to detect unwanted time-variation.

\subsection{Optimizing Assembly Routines}

Currently, we rely on some assembly routines pulled from
\texttt{big.Int}, slightly modified to avoid jumping to non-constant-time
routines. Unfortunately, not all of the primitive operations we would
like to have are present. Furthermore, we could reduce memory usage
in some places, by having these operations present a "conditional"
variant. For example, we could have an add operation taking a
\texttt{choice} flag, allowing us to choose whether or not to perform
an addition, without leaking information. This would avoid having
to use a scratch buffer and a conditional copy.

To gain similar speed to the other primitives, these new primitives
would also need to be implemented in assembly. This would be time-consuming,
but likely worth the effort. There are also new solutions
to help with writing assembly routines in Go, such as the Avo library.

\subsection{Upstreaming to \texttt{go/crypto}}

While we believe our library is immediately useful for the broader
ecosystem, it's not realistically going to be replacing
the use of \texttt{big.Int} in Go's cryptography library any time soon.

The most likely path towards removing \texttt{big.Int} from
\texttt{go/crypto} is likely to move towards specialized arithmetic
implementations for each prime field involved in ECC. DSA is a legacy
algorithm, where the security flaws introduced
by \texttt{big.Int} are not of major concern.

This leaves RSA. Unfortunately, the nature of RSA, requiring dynamic
moduli, makes it so that a big number library of some kind is necessary.
Ideally, this library would be internal to RSA, allowing constant-time
operation, and severing the bridge between Go's cryptography
package, and \texttt{big.Int}.

As a proof of concept, we've implemented a fork of Go's RSA implementation
\cite{meier_cronokirbyctrsa_nodate},
replacing \texttt{big.Int} for encryption and decryption, 
with an internal number type, using the minimal amount of code
necessary to accomplish this.

Using unsaturated limbs, we've found that our fork of RSA
suffers only a 1.67x slowdown, while implementing encryption and
decryption in constant-time.

We hope to prepare a patch for Go's RSA implementation to merge
in this work soon.

\section{Conclusion}

In summary, we have shown
why Go's general purpose big number type, \texttt{big.Int},
is not suitable for Cryptography, for various reasons.
Unfortunately, this type gets used out of convenience,
even in Go's own cryptography library.

To address this, we've created a replacement library for \texttt{big.Int},
achieving a slowdown of only 2.6x for most operations, while attempting
to provide constant-time operation.

To test the utility of this library, we've replaced the usage
of \texttt{big.Int} in Go's implementation of RSA, and DSA, and found
only a slowdown of 2x.

\section*{Acknowledgements}

Firstly, I'd like to thank Professor Bryan Ford for supervising this work.
I'd like to thank Pierluca Borsò as well, for letting me 
work on this project.
Finally, I'd also like to thank Daniel Huigens, and Marin Thiercelin,
from ProtonMail, for their advice and industry perspective on this work.

\addcontentsline{toc}{section}{Acknowledgements}

\bibliographystyle{plainurl}
\bibliography{references}
\end{document}
